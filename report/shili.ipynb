{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    title  \\\n",
      "0                    Underestimating AAPL   \n",
      "1  Are you worried about AAPL long term?    \n",
      "2                     Sell or Hold $AAPL?   \n",
      "3                          I SOLD AAPL :(   \n",
      "4                 AAPL is at 52-week low!   \n",
      "\n",
      "                                                text  \n",
      "0  underestimating aapl im not at surprised at th...  \n",
      "1  are you worried about aapl long term  now im n...  \n",
      "2  sell or hold aapl im up  already almost k in g...  \n",
      "3  i sold aapl  i know  you just buy and forget i...  \n",
      "4  aapl is at week low hi guys\\n\\ni just notice t...  \n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 设置Reddit API\n",
    "reddit = praw.Reddit(client_id='ByGHuaBLiK2AdpNTPWKlCA',\n",
    "                     client_secret='KfB9LAgGXaJ7PhUzRFvNZr32P3g5lg',\n",
    "                     user_agent='Haibo Fang')\n",
    "\n",
    "# 获取Reddit帖子\n",
    "def get_reddit_posts(subreddit, query, limit=3000):\n",
    "    subreddit = reddit.subreddit(subreddit)\n",
    "    posts = []\n",
    "    for post in subreddit.search(query, limit=limit):\n",
    "        posts.append([post.title, post.selftext])\n",
    "    return posts\n",
    "\n",
    "# 数据清洗\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# 获取和清洗数据\n",
    "posts = get_reddit_posts('stocks', 'AAPL', limit=3000)\n",
    "df = pd.DataFrame(posts, columns=['title', 'text'])\n",
    "df['text'] = df['title'] + ' ' + df['text']\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    title  \\\n",
      "0                    Underestimating AAPL   \n",
      "1  Are you worried about AAPL long term?    \n",
      "2                     Sell or Hold $AAPL?   \n",
      "3                          I SOLD AAPL :(   \n",
      "4                 AAPL is at 52-week low!   \n",
      "\n",
      "                                                text  label  \n",
      "0  underestimating aapl im surprised kneejerk sma...      1  \n",
      "1  worried aapl long term im saying sell apple st...      0  \n",
      "2  sell hold aapl im already almost k gains finan...      1  \n",
      "3  sold aapl know buy forget yes know yet dumb ho...      1  \n",
      "4  aapl week low hi guys notice aapl near week lo...      0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\48869/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\48869/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 数据标注 (假设已经有标注的数据，实际情况需要手动标注或使用预标注的数据)\n",
    "df['label'] = [1 if 'good' in text or 'up' in text or 'rise' in text else 0 for text in df['text']]\n",
    "\n",
    "# 分词和编码\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "print(df.head())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 181, Testing samples: 46\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 训练集和测试集划分\n",
    "X = df['text'].values\n",
    "y = df['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}, Testing samples: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 181, Testing samples: 46\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 训练集和测试集划分\n",
    "X = df['text'].values\n",
    "y = df['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}, Testing samples: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded and padded training samples: (181, 150), Testing samples: (46, 150)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 文本编码和填充\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_len = 150\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=max_len)\n",
    "\n",
    "print(f\"Encoded and padded training samples: {X_train.shape}, Testing samples: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 18s 3s/step - loss: 0.6963 - accuracy: 0.4938 - val_loss: 0.6844 - val_accuracy: 0.6316\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6866 - accuracy: 0.5741 - val_loss: 0.6823 - val_accuracy: 0.6316\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.6824 - accuracy: 0.5741 - val_loss: 0.6769 - val_accuracy: 0.6316\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6728 - accuracy: 0.5741 - val_loss: 0.6692 - val_accuracy: 0.6316\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6464 - accuracy: 0.6296 - val_loss: 0.6301 - val_accuracy: 0.6842\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5848 - accuracy: 0.7346 - val_loss: 0.6181 - val_accuracy: 0.5789\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 10s 4s/step - loss: 0.5517 - accuracy: 0.7037 - val_loss: 0.4879 - val_accuracy: 0.6842\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.4980 - accuracy: 0.8148 - val_loss: 0.4852 - val_accuracy: 0.6842\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4578 - accuracy: 0.8580 - val_loss: 0.5045 - val_accuracy: 0.7368\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 9s 4s/step - loss: 0.3745 - accuracy: 0.8951 - val_loss: 0.4804 - val_accuracy: 0.6842\n",
      "2/2 [==============================] - 0s 250ms/step - loss: 0.6633 - accuracy: 0.6087\n",
      "Test Accuracy: 0.6086956262588501\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 构建LSTM模型\n",
    "def create_model(embedding_dim, spatial_dropout, lstm_units, dropout, recurrent_dropout):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=10000, output_dim=embedding_dim, input_length=max_len))\n",
    "    model.add(SpatialDropout1D(spatial_dropout))\n",
    "    model.add(Bidirectional(LSTM(lstm_units, dropout=dropout, recurrent_dropout=recurrent_dropout)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model(128, 0.3, 100, 0.3, 0.3)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "# 模型评估\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# 示例预测\n",
    "def predict_sentiment(text):\n",
    "    text = clean_text(text)\n",
    "    text = preprocess_text(text)\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(seq, padding='post', maxlen=max_len)\n",
    "    pred = model.predict(padded)\n",
    "    return 'Positive' if pred > 0.5 else 'Negative'\n",
    "\n",
    "example_text = \"The stock market is expected to rise\"\n",
    "print(predict_sentiment(example_text))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
